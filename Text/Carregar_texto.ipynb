{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "starting-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "#import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "turned-renewal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
      "6053888/6053168 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
    "dataset = utils.get_file('stack_overflow_16k.tar.gz',data_url,untar=True,cache_dir='stack_overflow',cache_subdir='')\n",
    "dataset_dir = pathlib.Path(dataset).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "premier-survival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('/tmp/.keras/README.md'),\n",
       " WindowsPath('/tmp/.keras/stack_overflow_16k.tar.gz.tar.gz'),\n",
       " WindowsPath('/tmp/.keras/test'),\n",
       " WindowsPath('/tmp/.keras/train')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "disciplinary-converter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('/tmp/.keras/train/csharp'),\n",
       " WindowsPath('/tmp/.keras/train/java'),\n",
       " WindowsPath('/tmp/.keras/train/javascript'),\n",
       " WindowsPath('/tmp/.keras/train/python')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = dataset_dir/'train'\n",
    "test_dir = dataset_dir/'test'\n",
    "\n",
    "list(train_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "economic-sympathy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why does this blank program print true x=true.def stupid():.    x=false.stupid().print x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_file = train_dir/'python/1755.txt'\n",
    "with open(sample_file) as f:\n",
    "  print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rational-majority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = preprocessing.text_dataset_from_directory(train_dir,batch_size=batch_size,validation_split=0.2,\n",
    "                                                         subset='training',seed=seed)\n",
    "raw_val_ds = preprocessing.text_dataset_from_directory(train_dir,batch_size=batch_size,validation_split=0.2,\n",
    "                                                       subset='validation',seed=seed)\n",
    "\n",
    "raw_test_ds = preprocessing.text_dataset_from_directory(test_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "occupational-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n",
      "Label: 3\n",
      "Label: 2\n",
      "Label: 0\n",
      "Label: 0\n",
      "Label: 0\n",
      "Label: 2\n",
      "Label: 0\n",
      "Label: 2\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(10):\n",
    "    #print(\"\\nQuestion: \", text_batch.numpy()[i])\n",
    "    print(\"Label:\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "critical-expense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to csharp\n",
      "Label 1 corresponds to java\n",
      "Label 2 corresponds to javascript\n",
      "Label 3 corresponds to python\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(raw_train_ds.class_names):\n",
    "  print(\"Label\", i, \"corresponds to\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "asian-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(max_tokens=VOCAB_SIZE,output_mode='binary')\n",
    "\n",
    "int_vectorize_layer = TextVectorization(max_tokens=VOCAB_SIZE,output_mode='int',\n",
    "                                        output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "boring-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "binary_vectorize_layer.adapt(train_text)\n",
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "accompanied-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return binary_vectorize_layer(text), label\n",
    "\n",
    "def int_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return int_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dynamic-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question tf.Tensor(b'\"set blank to quit on exception? i\\'m using blank 3..i\\'ve been looking around for an answer to this, but i haven\\'t found it yet. basically, i\\'m running several blank scripts into a game engine, and each script has its own entry point...i\\'d rather not add try: except blocks through all of my code, so i was wondering if it\\'s at all possible to tell blank to quit (or perhaps assign a custom function to that \"\"callback\"\") on finding its first error, regardless of where or what it found? ..currently, the game engine will continue after finding and hitting an error, making it more difficult than necessary to diagnose issues since running into one error may make a subsequent script not work (as it relies on variables that the error-ing script set, for example). any ideas? ..i know that i could redirect the console to a file to allow for easier scrolling, but just capturing the first error and stopping the game prematurely would be really useful...okay, a couple of extra bits of info - sorry for neglecting to say this. the engine i\\'m using (the blender game engine) is coded in c, so changing the source is more than i\\'d like to do.....after googling, it would appear that a similar question with a solid answer has been asked here, which is how to get the last raised exception. if i check the sys module for the presence of the last_value variable and it exists, then i can quit prematurely, as the console would have already printed out the error...thanks for the help.\"\\n', shape=(), dtype=string)\n",
      "Label tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_question, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Question\", first_question)\n",
    "print(\"Label\", first_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sapphire-judgment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'binary' vectorized question: tf.Tensor([[1. 1. 1. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n",
      "'int' vectorized question: tf.Tensor(\n",
      "[[ 107   16    4 1138   38  184   52   47   16    1  215  416  518   12\n",
      "    32  181    4   13   26    3 1414  227   11  894  627   52  309  718\n",
      "    16 1272  100    5  244 1826    8  119  223   95   96  657  858    1\n",
      "   797   20  125  117  559 2280  194   73    9   23   30   49    3  114\n",
      "   566   10   96   59   73  206    4  412   16    4 1138   45 1669  600\n",
      "     5  696   37    4   14  850   38  966   96   98   65 2373    9  132\n",
      "    45   55   11  227  403    2  244 1826   72  534  156  966    8 7799\n",
      "    32   65  469   11  182 2332  198 1460    4    1 1093  447  309  100\n",
      "    71   65  454  109    5 3554  223   20  139   36   11 9038   38  233\n",
      "    14    2    1  223  107   12  137   76  778    3  103   14    3  177\n",
      "  1852    2  332    4    5   39    4  787   12 1319 3573   26  106    1\n",
      "     2   98   65    8 2900    2  244    1   69   33  336    1    5 1758\n",
      "     9 1157 2528    9  568  729   12    1    4  321   13    2 1826   52\n",
      "    47    2    1  244 1826    6 2616    7  131   49  801    2  313    6\n",
      "   182  198  142   48    4    1 3958   11   69  881   14    5  522  159\n",
      "    21    5 1705  181   95  215  912  101   66    6   24    4   41    2\n",
      "   218 2916  184   10    3  186    2 1358  380   12    2 9137    9    2\n",
      "     1  116    8   11  756   87    3   35 1138    1   36    2]], shape=(1, 250), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(\"'binary' vectorized question:\", binary_vectorize_text(first_question, first_label)[0])\n",
    "print(\"'int' vectorized question:\",int_vectorize_text(first_question, first_label)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "animated-drink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 --->  set\n",
      "16 --->  blank\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"107 ---> \", int_vectorize_layer.get_vocabulary()[107])\n",
    "print(\"16 ---> \", int_vectorize_layer.get_vocabulary()[16])\n",
    "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "absolute-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
    "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
    "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
    "\n",
    "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
    "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
    "int_test_ds = raw_test_ds.map(int_vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dated-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "charitable-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_ds = configure_dataset(binary_train_ds)\n",
    "binary_val_ds = configure_dataset(binary_val_ds)\n",
    "binary_test_ds = configure_dataset(binary_test_ds)\n",
    "\n",
    "int_train_ds = configure_dataset(int_train_ds)\n",
    "int_val_ds = configure_dataset(int_val_ds)\n",
    "int_test_ds = configure_dataset(int_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interesting-papua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 15s 71ms/step - loss: 1.2445 - accuracy: 0.5153 - val_loss: 0.9166 - val_accuracy: 0.7738\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8225 - accuracy: 0.8145 - val_loss: 0.7519 - val_accuracy: 0.7956\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6542 - accuracy: 0.8600 - val_loss: 0.6659 - val_accuracy: 0.8131\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5540 - accuracy: 0.8851 - val_loss: 0.6123 - val_accuracy: 0.8181\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.8996 - val_loss: 0.5755 - val_accuracy: 0.8263\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4323 - accuracy: 0.9170 - val_loss: 0.5489 - val_accuracy: 0.8306\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.9266 - val_loss: 0.5288 - val_accuracy: 0.8338\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3566 - accuracy: 0.9325 - val_loss: 0.5132 - val_accuracy: 0.8375\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.9381 - val_loss: 0.5010 - val_accuracy: 0.8388\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3028 - accuracy: 0.9440 - val_loss: 0.4913 - val_accuracy: 0.8413\n"
     ]
    }
   ],
   "source": [
    "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
    "binary_model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "history = binary_model.fit(binary_train_ds, validation_data=binary_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "headed-processing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4)                 40004     \n",
      "=================================================================\n",
      "Total params: 40,004\n",
      "Trainable params: 40,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "binary_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "loose-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels):\n",
    "  model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incident-crossing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 1.3177 - accuracy: 0.3981 - val_loss: 0.8071 - val_accuracy: 0.6844\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.7332 - accuracy: 0.7056 - val_loss: 0.5522 - val_accuracy: 0.7944\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.4410 - accuracy: 0.8533 - val_loss: 0.4702 - val_accuracy: 0.8175\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.2464 - accuracy: 0.9423 - val_loss: 0.4624 - val_accuracy: 0.8263\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.1243 - accuracy: 0.9768 - val_loss: 0.4811 - val_accuracy: 0.8288\n"
     ]
    }
   ],
   "source": [
    "# vocab_size is VOCAB_SIZE + 1 since 0 is used additionally for padding.\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "consolidated-compensation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model on binary vectorized data:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4)                 40004     \n",
      "=================================================================\n",
      "Total params: 40,004\n",
      "Trainable params: 40,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear model on binary vectorized data:\")\n",
    "print(binary_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "proprietary-shoot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet model on int vectorized data:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          640064    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 64)          20544     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 660,868\n",
      "Trainable params: 660,868\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"ConvNet model on int vectorized data:\")\n",
    "print(int_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "accompanied-volume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 50s 195ms/step - loss: 0.5190 - accuracy: 0.8141\n",
      "250/250 [==============================] - 4s 15ms/step - loss: 0.5203 - accuracy: 0.8069\n",
      "Binary model accuracy: 81.41%\n",
      "Int model accuracy: 80.69%\n"
     ]
    }
   ],
   "source": [
    "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
    "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
    "\n",
    "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
    "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-board",
   "metadata": {},
   "source": [
    "# Exporte o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-influence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
